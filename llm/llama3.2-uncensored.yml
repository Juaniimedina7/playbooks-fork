settings:
  name: "Llama 3.2 uncensored: Query this LLM"
  description: "Queries Llama 3.2 uncensored with the provided input. Ensures the Ollama server runs correctly, pulls the specified model, and executes the query."
  image: ollama/ollama
  cpu: 16384
  memory: 122880
  timeout: 400
  author:
    - https://github.com/ollama
  gallery:
    - https://files.catbox.moe/u5fjdn.png
  example: satori run satori://llm/llama3.2-uncensored.yml -d INPUT="Hello World" --report --output

MODEL:
  - - "artifish/llama3.2-uncensored"

llama3_2-uncensored:
  install:
    - apt-get update >> /dev/null; apt-get install -qy screen >> /dev/null

  test:
    assertReturnCode: 0
    serve:
      - ps aux | grep -v grep | grep -q ollama || screen -dm ollama serve
      - sleep 1

    pull:
      - ollama pull ${{MODEL}}

    query:
      - ollama run ${{MODEL}} "${{INPUT}}"
