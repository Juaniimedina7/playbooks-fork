settings:
  name: "Query a repository with Llama 3.2 using Ollama"
  image: ollama/ollama
  cpu: 16384
  memory: 122880
  timeout: 400
  example: satori run satori://llm/ollama/repo.yml -d INPUT="what payload overflows the name to write 69 on the age on the example.c program?" -d REPO="royleekiat/overflow-example" --output

MODEL:
- - "artifish/llama3.2-uncensored"

install:
  dependencies:
  - apt update >> /dev/null
  - apt install -qy screen jq curl git >> /dev/null

prompt:
  #- echo "${{INPUT}}" > /tmp/prompt.txt
  - git clone https://github.com/${{REPO}}.git .
  - echo "I will provide files and their contents to use as reference for a query that I will provide at the end:" >> /tmp/prompt.txt
  - bash -c 'find . -type f -print0 | while IFS= read -r -d "" f; do echo "$f"; cat "$f"; echo; done' >> /tmp/prompt.txt
  - echo "${{INPUT}}" > /tmp/prompt.txt

ollama:
  serve:
  - screen -dm ollama serve; sleep 1
  pull:
  - ollama pull ${{MODEL}}
  query:
  - ollama run ${{MODEL}} "$(cat /tmp/prompt.txt)"
