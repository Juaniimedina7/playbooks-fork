settings:
  name: "LLMs with parametrized input: run on Llama3 and Mistral"
  description: "Runs a parametrized input on Llama3 and Mistral using Ollama. Starts the Ollama server, pulls the specified models, and queries them with the provided input. Extracts the response using a local API call and processes the output."
  image: ollama/ollama
  cpu: 16384
  memory: 122880
  timeout: 300
  author:
    - https://github.com/ollama
  example: satori run satori://llm/ollama/llama3_mistral.yml -d INPUT="Hello World" --report --output

MODEL:
  - - "llama3"
    - "mistral"

ollama:
  install:
    - apt update >> /dev/null; apt install -qy screen jq curl >> /dev/null

  test:
    assertReturnCode: 0
    serve:
      - screen -dm ollama serve; sleep 1

    pull:
      - ollama pull ${{MODEL}}

    generate:
      - "curl -s http://localhost:11434/api/generate -d '{\"model\": \"${{MODEL}}\",\"prompt\":\"${{INPUT}}\"}' | grep \"model.*response\" | jq -r 'select(.done==false) | .response' | tr -d '\\n'"
