settings:
  name: "Cloud Scraper: enumerates targets in search of cloud resources (S3 Buckets, Azure Blobs, Digital Ocean Storage Space)"
  description: "CloudScraper spiders and scrapes target websites to identify exposed cloud resources, such as AWS S3 buckets, Azure Blobs, and DigitalOcean Spaces. By inputting a URL, it recursively searches through the siteâ€™s pages, extracting links and scanning for patterns indicative of cloud storage locations."
  image: python
  author:
    - https://github.com/jordanpotti
  gallery:
    - https://files.catbox.moe/c4q8hc.gif
  example: satori run satori://web/cloudscraper.yml -d URL="http://example.com" --report --output

cloudscraper:
  install:
    - apt-get update >>/dev/null; apt-get install -qy git >>/dev/null
    - cd /tmp; git clone https://github.com/jordanpotti/CloudScraper.git --depth 1; pip3 install -r CloudScraper/requirements.txt -q --disable-pip-version-check

  help:
    - python3 /tmp/CloudScraper/CloudScraper.py --help
  test:
    assertStdoutContains: "There were no matches!"
    run:
      - python3 /tmp/CloudScraper/CloudScraper.py -u ${{URL}}
